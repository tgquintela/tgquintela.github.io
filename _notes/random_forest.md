---
title: "Random Forest"
collection: notes
permalink: /notes/random_forest
date: 2016-06-01
Tags:
  - Artificial Intelligence
  - Machine Learning
---

Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.

The first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.

An extension of the algorithm was developed by Leo Breiman and Adele Cutler, and "Random Forests" is their trademark. The extension combines Breiman's "bagging" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.


#### See also
[Machine Learning](/notes/machine_learning), [Artificial Neural Networks](/notes/artificial_neural_networks), [Bagging](/notes/bagging)




## Papers
* Liaw, A., & Wiener, M. (2002). [Classification and regression by randomForest](ftp://131.252.97.79/Transfer/Treg/WFRE_Articles/Liaw_02_Classification%20and%20regression%20by%20randomForest.pdf). R news, 2(3), 18-22.
* Breiman, L. (2001). [Random forests](http://machinelearning202.pbworks.com/w/file/fetch/60606349/breiman_randomforests.pdf). Machine learning, 45(1), 5-32.
* Segal, M. R. (2004). [Machine learning benchmarks and random forest regression](https://escholarship.org/uc/item/35x3v9t4.pdf). Center for Bioinformatics & Molecular Biostatistics.




